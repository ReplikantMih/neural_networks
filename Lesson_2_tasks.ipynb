{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.datasets import mnist\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li>Попробуйте обучить, нейронную сеть на Keras(рассмотренную на уроке) на датасете MNIST с другими параметрами. \n",
    "        Опишите в комментарии к уроку - какой результата вы добились от нейросети? Что помогло вам улучшить ее точность?</li>\n",
    "    <li>Поработайте с документацией Keras. Попробуйте найти полезные команды Keras неразобранные на уроке.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведу ряд эксперименов, посмотрю влияние разных факторов на качество модели, а так же ряд других экспериментов. Далее заголовками идут условные названия экспериментов, которые потом являются ключами в словаре итоговых метрик experiments_results - поэтому названия именно такого плана (чтобы можно было можно было в качестве ключа указать).\n",
    "\n",
    "В экспериментах по оценке влияния факторов будет изменен только оцениваемый параметр/фактор, либо несколько, если эксперимент предполагает оценку совместного влияния. Все изменения происходят относительно варианта baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем данные и преобразуем в готовый для обучения вид."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = (train_images / 255) - 0.5\n",
    "test_images = (test_images / 255) - 0.5\n",
    "\n",
    "train_images = train_images.reshape((-1, 784))\n",
    "test_images = test_images.reshape((-1, 784))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомогательные методы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение модели с заданными параметрами обучения, возвращает метрику качества.\n",
    "def learn_evaluate(model,\n",
    "                   data: tuple,  # X_train, X_test, y_train, y_test.\n",
    "                   optimizer,\n",
    "                   loss_func,\n",
    "                   metrics,\n",
    "                   epochs,\n",
    "                   batch_size):\n",
    "    model.compile(\n",
    "      optimizer=optimizer,\n",
    "      loss=loss_func,\n",
    "      metrics=metrics,\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "      data[0],\n",
    "      to_categorical(data[2]),\n",
    "      epochs=epochs,\n",
    "      batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    eval_ = model.evaluate(\n",
    "      data[1],\n",
    "      to_categorical(data[3])\n",
    "    )\n",
    "\n",
    "    return eval_[-1]\n",
    "\n",
    "\n",
    "# Многократное обучение модели с заданными параметрами, возваращает среднее значение метрики качества и std.\n",
    "def learn_evaluate_n_times(n_times,\n",
    "                           model,\n",
    "                           data: tuple,  # X_train, X_test, y_train, y_test.\n",
    "                           optimizer,\n",
    "                           loss_func,\n",
    "                           metrics,\n",
    "                           epochs,\n",
    "                           batch_size):\n",
    "    result_metrics = []\n",
    "    for i in range(n_times):\n",
    "        result = learn_evaluate(model,\n",
    "                              data,\n",
    "                              optimizer,\n",
    "                              loss_func,\n",
    "                              metrics,\n",
    "                              epochs,\n",
    "                              batch_size)\n",
    "\n",
    "        result_metrics.append(result)\n",
    "    result_metrics = np.array(result_metrics)\n",
    "    return {'mean': round(result_metrics.mean(), 4), 'std': round(result_metrics.std(), 4)}\n",
    "\n",
    "\n",
    "# Обучение модели и получение предикта по тестовой выборке.\n",
    "def learn_predict(model,\n",
    "                   data: tuple,  # X_train, X_test, y_train, y_test.\n",
    "                   optimizer,\n",
    "                   loss_func,\n",
    "                   metrics,\n",
    "                   epochs,\n",
    "                   batch_size):\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss_func,\n",
    "        metrics=metrics,\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        data[0],\n",
    "        to_categorical(data[2]),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    eval_ = model.evaluate(\n",
    "        data[1],\n",
    "        to_categorical(data[3])\n",
    "    )\n",
    "    \n",
    "    return model.predict(data[1])\n",
    "\n",
    "\n",
    "# Метрики качества по разным экспериментам.\n",
    "experiments_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### baseline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Базовая модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.3592 - accuracy: 0.8916\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.1860 - accuracy: 0.9431\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.1418 - accuracy: 0.9567\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.1195 - accuracy: 0.9627\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 0.1066 - accuracy: 0.9665\n",
      "10000/10000 [==============================] - 1s 74us/step\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.0940 - accuracy: 0.9703\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.0870 - accuracy: 0.9728\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.0792 - accuracy: 0.9746\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.0716 - accuracy: 0.9771\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.0688 - accuracy: 0.9778\n",
      "10000/10000 [==============================] - 1s 75us/step\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.0655 - accuracy: 0.9790\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 0.0603 - accuracy: 0.9803\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 5s 75us/step - loss: 0.0564 - accuracy: 0.9812\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.0538 - accuracy: 0.9821\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 0.0505 - accuracy: 0.9833\n",
      "10000/10000 [==============================] - 1s 75us/step\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.0487 - accuracy: 0.9840\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.0486 - accuracy: 0.9836\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.0421 - accuracy: 0.9854\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.0444 - accuracy: 0.9844\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.0401 - accuracy: 0.9865\n",
      "10000/10000 [==============================] - 1s 75us/step\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.0385 - accuracy: 0.9869\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.0362 - accuracy: 0.9875\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.0352 - accuracy: 0.9881\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.0363 - accuracy: 0.9876\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.0312 - accuracy: 0.9892\n",
      "10000/10000 [==============================] - 1s 75us/step\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "  Dense(64, activation='relu', input_shape=(784,)),\n",
    "  Dense(64, activation='relu'),\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "\n",
    "experiments_results['baseline'] =\\\n",
    "learn_evaluate_n_times(n_times=5,\n",
    "               model=model,\n",
    "               data=(train_images, test_images, train_labels, test_labels),\n",
    "               optimizer='adam',\n",
    "               loss_func='categorical_crossentropy',\n",
    "               metrics=['accuracy'],\n",
    "               epochs=5,\n",
    "               batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'baseline': {'mean': 0.9713, 'std': 0.0021}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### more_layers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Больше слоев."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.3530 - accuracy: 0.8905\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.1766 - accuracy: 0.9464\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1408 - accuracy: 0.9565\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1241 - accuracy: 0.9611\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.1083 - accuracy: 0.9657\n",
      "10000/10000 [==============================] - 0s 18us/step\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0988 - accuracy: 0.9685\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.0919 - accuracy: 0.9707\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.0824 - accuracy: 0.9739\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.0766 - accuracy: 0.9755\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.0720 - accuracy: 0.9765\n",
      "10000/10000 [==============================] - 0s 19us/step\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.0678 - accuracy: 0.9780\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.0628 - accuracy: 0.9796\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.0612 - accuracy: 0.9800\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.0564 - accuracy: 0.9815\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.0553 - accuracy: 0.9821\n",
      "10000/10000 [==============================] - 0s 20us/step\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.0526 - accuracy: 0.9827\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.0490 - accuracy: 0.9843\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.0461 - accuracy: 0.9848\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0472 - accuracy: 0.9843\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.0424 - accuracy: 0.9863\n",
      "10000/10000 [==============================] - 0s 22us/step\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 2s 39us/step - loss: 0.0427 - accuracy: 0.9860\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0415 - accuracy: 0.9862\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0393 - accuracy: 0.9870\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.0394 - accuracy: 0.9876\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.0367 - accuracy: 0.9881\n",
      "10000/10000 [==============================] - 0s 22us/step\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "  Dense(64, activation='relu', input_shape=(784,)),\n",
    "  Dense(64, activation='relu'),\n",
    "  Dense(64, activation='relu'),\n",
    "  Dense(64, activation='relu'),\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "experiments_results['more_layers'] =\\\n",
    "learn_evaluate_n_times(n_times=5,\n",
    "               model=model,\n",
    "               data=(train_images, test_images, train_labels, test_labels),\n",
    "               optimizer='adam',\n",
    "               loss_func='categorical_crossentropy',\n",
    "               metrics=['accuracy'],\n",
    "               epochs=5,\n",
    "               batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'baseline': {'mean': 0.9713, 'std': 0.0021},\n",
       " 'more_layers': {'mean': 0.9678, 'std': 0.0038}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### more_neurones_in_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Больше нейронов в одном слое."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.3129 - accuracy: 0.9047\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.1516 - accuracy: 0.9524\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.1140 - accuracy: 0.9642\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.0946 - accuracy: 0.9699\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.0814 - accuracy: 0.9742\n",
      "10000/10000 [==============================] - 0s 24us/step\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 3s 53us/step - loss: 0.0766 - accuracy: 0.9753\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 3s 50us/step - loss: 0.0632 - accuracy: 0.9794\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.0579 - accuracy: 0.9809\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.0526 - accuracy: 0.9823\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.0493 - accuracy: 0.9834\n",
      "10000/10000 [==============================] - 0s 25us/step\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 3s 50us/step - loss: 0.0466 - accuracy: 0.9844\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.0418 - accuracy: 0.9860\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.0397 - accuracy: 0.9871\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.0375 - accuracy: 0.9875\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.0332 - accuracy: 0.9882\n",
      "10000/10000 [==============================] - 0s 26us/step\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.0349 - accuracy: 0.9880\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.0305 - accuracy: 0.9905\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.0292 - accuracy: 0.9902\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.0295 - accuracy: 0.9904\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.0283 - accuracy: 0.9909\n",
      "10000/10000 [==============================] - 0s 28us/step\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.0303 - accuracy: 0.9902\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 3s 52us/step - loss: 0.0248 - accuracy: 0.9918\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 3s 52us/step - loss: 0.0241 - accuracy: 0.9920\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.0238 - accuracy: 0.9922\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 3s 52us/step - loss: 0.0224 - accuracy: 0.9926\n",
      "10000/10000 [==============================] - 0s 27us/step\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "  Dense(128, activation='relu', input_shape=(784,)),\n",
    "  Dense(128, activation='relu'),\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "experiments_results['more_neurones_in_layer'] =\\\n",
    "learn_evaluate_n_times(n_times=5,\n",
    "               model=model,\n",
    "               data=(train_images, test_images, train_labels, test_labels),\n",
    "               optimizer='adam',\n",
    "               loss_func='categorical_crossentropy',\n",
    "               metrics=['accuracy'],\n",
    "               epochs=5,\n",
    "               batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'baseline': {'mean': 0.9713, 'std': 0.0021},\n",
       " 'more_layers': {'mean': 0.9678, 'std': 0.0038},\n",
       " 'more_neurones_in_layer': {'mean': 0.9728, 'std': 0.0042}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### more_neurones_in_layer_and_more_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одновременно больше нейронов в слое и больше слоев."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.3143 - accuracy: 0.9002\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.1618 - accuracy: 0.9501\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.1281 - accuracy: 0.9597\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.1072 - accuracy: 0.9663\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.0933 - accuracy: 0.9712\n",
      "10000/10000 [==============================] - 0s 31us/step\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.0848 - accuracy: 0.9734\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.0756 - accuracy: 0.9761\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.0688 - accuracy: 0.9784\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.0624 - accuracy: 0.9804\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.0568 - accuracy: 0.9818\n",
      "10000/10000 [==============================] - 0s 32us/step\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.0549 - accuracy: 0.9827\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.0505 - accuracy: 0.9837\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.0489 - accuracy: 0.9842\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.0458 - accuracy: 0.9852\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.0427 - accuracy: 0.9863\n",
      "10000/10000 [==============================] - 0s 33us/step\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.0443 - accuracy: 0.9863\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0416 - accuracy: 0.9872\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0390 - accuracy: 0.9879\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0366 - accuracy: 0.9887\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0374 - accuracy: 0.9882\n",
      "10000/10000 [==============================] - 0s 35us/step\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.0394 - accuracy: 0.9886\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.0327 - accuracy: 0.9894\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.0342 - accuracy: 0.9897\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.0327 - accuracy: 0.9901\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.0346 - accuracy: 0.9895\n",
      "10000/10000 [==============================] - 0s 36us/step\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "  Dense(128, activation='relu', input_shape=(784,)),\n",
    "  Dense(128, activation='relu'), \n",
    "  Dense(128, activation='relu'),\n",
    "  Dense(128, activation='relu'),\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "experiments_results['more_neurones_in_layer_and_more_layers'] =\\\n",
    "learn_evaluate_n_times(n_times=5,\n",
    "               model=model,\n",
    "               data=(train_images, test_images, train_labels, test_labels),\n",
    "               optimizer='adam',\n",
    "               loss_func='categorical_crossentropy',\n",
    "               metrics=['accuracy'],\n",
    "               epochs=5,\n",
    "               batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'baseline': {'mean': 0.9713, 'std': 0.0021},\n",
       " 'more_layers': {'mean': 0.9678, 'std': 0.0038},\n",
       " 'more_neurones_in_layer': {'mean': 0.9728, 'std': 0.0042},\n",
       " 'more_neurones_in_layer_and_more_layers': {'mean': 0.9713, 'std': 0.0043}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sigmoid_inside"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция активации внутреннего слоя заменена на sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.4583 - accuracy: 0.8775\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.2155 - accuracy: 0.9358\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.1638 - accuracy: 0.9506\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.1344 - accuracy: 0.9599\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.1163 - accuracy: 0.9643\n",
      "10000/10000 [==============================] - 0s 32us/step\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.1046 - accuracy: 0.9678\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.0924 - accuracy: 0.9719\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.0860 - accuracy: 0.9736\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.0775 - accuracy: 0.9758\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.0723 - accuracy: 0.9771\n",
      "10000/10000 [==============================] - 0s 35us/step\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.0691 - accuracy: 0.9781\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.0624 - accuracy: 0.9801\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.0592 - accuracy: 0.9812\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.0569 - accuracy: 0.9822\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.0517 - accuracy: 0.9839\n",
      "10000/10000 [==============================] - 0s 36us/step\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.0517 - accuracy: 0.9831\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.0469 - accuracy: 0.9856\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.0453 - accuracy: 0.9858\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.0435 - accuracy: 0.9859\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 2s 42us/step - loss: 0.0383 - accuracy: 0.9875\n",
      "10000/10000 [==============================] - 0s 35us/step\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.0381 - accuracy: 0.9877\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0371 - accuracy: 0.9878\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.0359 - accuracy: 0.9883\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0325 - accuracy: 0.9899\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.0309 - accuracy: 0.9895\n",
      "10000/10000 [==============================] - 0s 37us/step\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "  Dense(64, activation='relu', input_shape=(784,)),\n",
    "  Dense(64, activation='sigmoid'),\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "\n",
    "experiments_results['sigmoid_inside'] =\\\n",
    "learn_evaluate_n_times(n_times=5,\n",
    "               model=model,\n",
    "               data=(train_images, test_images, train_labels, test_labels),\n",
    "               optimizer='adam',\n",
    "               loss_func='categorical_crossentropy',\n",
    "               metrics=['accuracy'],\n",
    "               epochs=5,\n",
    "               batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'baseline': {'mean': 0.9713, 'std': 0.0021},\n",
       " 'more_layers': {'mean': 0.9678, 'std': 0.0038},\n",
       " 'more_neurones_in_layer': {'mean': 0.9728, 'std': 0.0042},\n",
       " 'more_neurones_in_layer_and_more_layers': {'mean': 0.9713, 'std': 0.0043},\n",
       " 'sigmoid_inside': {'mean': 0.9678, 'std': 0.0059}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### more_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Увеличено кол-во эпох обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.3623 - accuracy: 0.8903\n",
      "Epoch 2/15\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.1882 - accuracy: 0.9428\n",
      "Epoch 3/15\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.1452 - accuracy: 0.9556\n",
      "Epoch 4/15\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.1229 - accuracy: 0.9621\n",
      "Epoch 5/15\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.1042 - accuracy: 0.9678\n",
      "Epoch 6/15\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0944 - accuracy: 0.9705\n",
      "Epoch 7/15\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.0843 - accuracy: 0.9739\n",
      "Epoch 8/15\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0803 - accuracy: 0.9748\n",
      "Epoch 9/15\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0743 - accuracy: 0.9764\n",
      "Epoch 10/15\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0665 - accuracy: 0.9784\n",
      "Epoch 11/15\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0647 - accuracy: 0.9793\n",
      "Epoch 12/15\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0613 - accuracy: 0.9803\n",
      "Epoch 13/15\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0550 - accuracy: 0.9818\n",
      "Epoch 14/15\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0538 - accuracy: 0.9823\n",
      "Epoch 15/15\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0503 - accuracy: 0.9838\n",
      "10000/10000 [==============================] - 0s 38us/step\n",
      "Epoch 1/15\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.0491 - accuracy: 0.9840\n",
      "Epoch 2/15\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.0452 - accuracy: 0.9847\n",
      "Epoch 3/15\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0440 - accuracy: 0.9850\n",
      "Epoch 4/15\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0424 - accuracy: 0.9859\n",
      "Epoch 5/15\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0424 - accuracy: 0.9858\n",
      "Epoch 6/15\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0366 - accuracy: 0.9870\n",
      "Epoch 7/15\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0377 - accuracy: 0.9875\n",
      "Epoch 8/15\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0353 - accuracy: 0.9876\n",
      "Epoch 9/15\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0356 - accuracy: 0.9878\n",
      "Epoch 10/15\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0328 - accuracy: 0.9893\n",
      "Epoch 11/15\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0343 - accuracy: 0.9885\n",
      "Epoch 12/15\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0284 - accuracy: 0.9895\n",
      "Epoch 13/15\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0327 - accuracy: 0.9890\n",
      "Epoch 14/15\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0278 - accuracy: 0.9902\n",
      "Epoch 15/15\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0297 - accuracy: 0.9902\n",
      "10000/10000 [==============================] - 0s 38us/step\n",
      "Epoch 1/15\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.0296 - accuracy: 0.9901\n",
      "Epoch 2/15\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.0283 - accuracy: 0.9904\n",
      "Epoch 3/15\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.0275 - accuracy: 0.9907\n",
      "Epoch 4/15\n",
      "60000/60000 [==============================] - 3s 49us/step - loss: 0.0279 - accuracy: 0.9904\n",
      "Epoch 5/15\n",
      "60000/60000 [==============================] - 3s 49us/step - loss: 0.0242 - accuracy: 0.9911\n",
      "Epoch 6/15\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.0268 - accuracy: 0.9906\n",
      "Epoch 7/15\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.0238 - accuracy: 0.9918\n",
      "Epoch 8/15\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.0238 - accuracy: 0.9921\n",
      "Epoch 9/15\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.0243 - accuracy: 0.9919\n",
      "Epoch 10/15\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.0216 - accuracy: 0.9926\n",
      "Epoch 11/15\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0246 - accuracy: 0.9918\n",
      "Epoch 12/15\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0231 - accuracy: 0.9921\n",
      "Epoch 13/15\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0213 - accuracy: 0.9926\n",
      "Epoch 14/15\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.0212 - accuracy: 0.9927\n",
      "Epoch 15/15\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.0209 - accuracy: 0.9930\n",
      "10000/10000 [==============================] - 0s 39us/step\n",
      "Epoch 1/15\n",
      "60000/60000 [==============================] - 3s 49us/step - loss: 0.0220 - accuracy: 0.9929\n",
      "Epoch 2/15\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.0203 - accuracy: 0.9931\n",
      "Epoch 3/15\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.0195 - accuracy: 0.9929\n",
      "Epoch 4/15\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.0195 - accuracy: 0.9934\n",
      "Epoch 5/15\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.0196 - accuracy: 0.9935\n",
      "Epoch 6/15\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.0201 - accuracy: 0.9936\n",
      "Epoch 7/15\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.0192 - accuracy: 0.9937\n",
      "Epoch 8/15\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.0190 - accuracy: 0.9937\n",
      "Epoch 9/15\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.0203 - accuracy: 0.9938\n",
      "Epoch 10/15\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.0192 - accuracy: 0.9937\n",
      "Epoch 11/15\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.0210 - accuracy: 0.9931\n",
      "Epoch 12/15\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.0206 - accuracy: 0.9929\n",
      "Epoch 13/15\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.0164 - accuracy: 0.9944\n",
      "Epoch 14/15\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.0175 - accuracy: 0.9944\n",
      "Epoch 15/15\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.0185 - accuracy: 0.9940\n",
      "10000/10000 [==============================] - 0s 41us/step\n",
      "Epoch 1/15\n",
      "60000/60000 [==============================] - 3s 49us/step - loss: 0.0187 - accuracy: 0.9941\n",
      "Epoch 2/15\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.0178 - accuracy: 0.9943\n",
      "Epoch 3/15\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.0192 - accuracy: 0.9937\n",
      "Epoch 4/15\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.0167 - accuracy: 0.9946\n",
      "Epoch 5/15\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.0176 - accuracy: 0.9947\n",
      "Epoch 6/15\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.0175 - accuracy: 0.9944\n",
      "Epoch 7/15\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.0195 - accuracy: 0.9936\n",
      "Epoch 8/15\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.0176 - accuracy: 0.9948\n",
      "Epoch 9/15\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.0145 - accuracy: 0.9955\n",
      "Epoch 10/15\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.0183 - accuracy: 0.9941\n",
      "Epoch 11/15\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.0176 - accuracy: 0.9944\n",
      "Epoch 12/15\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.0170 - accuracy: 0.9951\n",
      "Epoch 13/15\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.0153 - accuracy: 0.9952\n",
      "Epoch 14/15\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.0159 - accuracy: 0.9951\n",
      "Epoch 15/15\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.0160 - accuracy: 0.9952\n",
      "10000/10000 [==============================] - 0s 40us/step\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "  Dense(64, activation='relu', input_shape=(784,)),\n",
    "  Dense(64, activation='relu'),\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "\n",
    "experiments_results['more_epochs'] =\\\n",
    "learn_evaluate_n_times(n_times=5,\n",
    "               model=model,\n",
    "               data=(train_images, test_images, train_labels, test_labels),\n",
    "               optimizer='adam',\n",
    "               loss_func='categorical_crossentropy',\n",
    "               metrics=['accuracy'],\n",
    "               epochs=15,\n",
    "               batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'baseline': {'mean': 0.9713, 'std': 0.0021},\n",
       " 'more_layers': {'mean': 0.9678, 'std': 0.0038},\n",
       " 'more_neurones_in_layer': {'mean': 0.9728, 'std': 0.0042},\n",
       " 'more_neurones_in_layer_and_more_layers': {'mean': 0.9713, 'std': 0.0043},\n",
       " 'sigmoid_inside': {'mean': 0.9678, 'std': 0.0059},\n",
       " 'more_epochs': {'mean': 0.9725, 'std': 0.0018}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### larger_batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Увеличено значение параметра batch_size метода fit()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.4703 - accuracy: 0.8641\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.2284 - accuracy: 0.9325\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1808 - accuracy: 0.9456\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1490 - accuracy: 0.9548\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1305 - accuracy: 0.9607\n",
      "10000/10000 [==============================] - 0s 42us/step\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.1176 - accuracy: 0.9638\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1025 - accuracy: 0.9687\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0929 - accuracy: 0.9715\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0846 - accuracy: 0.9737\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0787 - accuracy: 0.9755\n",
      "10000/10000 [==============================] - 0s 41us/step\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.0745 - accuracy: 0.9767\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0669 - accuracy: 0.9789\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0630 - accuracy: 0.9799\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0575 - accuracy: 0.9818\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0561 - accuracy: 0.9816\n",
      "10000/10000 [==============================] - 0s 45us/step\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0558 - accuracy: 0.9821\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0489 - accuracy: 0.9841\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0462 - accuracy: 0.9850\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0437 - accuracy: 0.9858\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0404 - accuracy: 0.9867\n",
      "10000/10000 [==============================] - 0s 45us/step\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0417 - accuracy: 0.9864\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0392 - accuracy: 0.9873\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0331 - accuracy: 0.9890\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0320 - accuracy: 0.9893\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0315 - accuracy: 0.9892\n",
      "10000/10000 [==============================] - 0s 46us/step\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "  Dense(64, activation='relu', input_shape=(784,)),\n",
    "  Dense(64, activation='relu'),\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "\n",
    "experiments_results['larger_batch_size'] =\\\n",
    "learn_evaluate_n_times(n_times=5,\n",
    "               model=model,\n",
    "               data=(train_images, test_images, train_labels, test_labels),\n",
    "               optimizer='adam',\n",
    "               loss_func='categorical_crossentropy',\n",
    "               metrics=['accuracy'],\n",
    "               epochs=5,\n",
    "               batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'baseline': {'mean': 0.9713, 'std': 0.0021},\n",
       " 'more_layers': {'mean': 0.9678, 'std': 0.0038},\n",
       " 'more_neurones_in_layer': {'mean': 0.9728, 'std': 0.0042},\n",
       " 'more_neurones_in_layer_and_more_layers': {'mean': 0.9713, 'std': 0.0043},\n",
       " 'sigmoid_inside': {'mean': 0.9678, 'std': 0.0059},\n",
       " 'more_epochs': {'mean': 0.9725, 'std': 0.0018},\n",
       " 'larger_batch_size': {'mean': 0.9691, 'std': 0.0033}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### less_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уменьшен размер обучающей выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5000/5000 [==============================] - 0s 98us/step - loss: 0.9079 - accuracy: 0.7426\n",
      "Epoch 2/5\n",
      "5000/5000 [==============================] - 0s 48us/step - loss: 0.4002 - accuracy: 0.8804\n",
      "Epoch 3/5\n",
      "5000/5000 [==============================] - 0s 49us/step - loss: 0.3287 - accuracy: 0.9028\n",
      "Epoch 4/5\n",
      "5000/5000 [==============================] - 0s 47us/step - loss: 0.2914 - accuracy: 0.9178\n",
      "Epoch 5/5\n",
      "5000/5000 [==============================] - 0s 48us/step - loss: 0.2556 - accuracy: 0.9252\n",
      "10000/10000 [==============================] - 0s 47us/step\n",
      "Epoch 1/5\n",
      "5000/5000 [==============================] - 1s 101us/step - loss: 0.2403 - accuracy: 0.9304\n",
      "Epoch 2/5\n",
      "5000/5000 [==============================] - 0s 49us/step - loss: 0.2168 - accuracy: 0.9368\n",
      "Epoch 3/5\n",
      "5000/5000 [==============================] - 0s 50us/step - loss: 0.1962 - accuracy: 0.9404\n",
      "Epoch 4/5\n",
      "5000/5000 [==============================] - 0s 50us/step - loss: 0.1703 - accuracy: 0.9498\n",
      "Epoch 5/5\n",
      "5000/5000 [==============================] - 0s 49us/step - loss: 0.1531 - accuracy: 0.9548\n",
      "10000/10000 [==============================] - 0s 46us/step\n",
      "Epoch 1/5\n",
      "5000/5000 [==============================] - 1s 102us/step - loss: 0.1546 - accuracy: 0.9552\n",
      "Epoch 2/5\n",
      "5000/5000 [==============================] - 0s 49us/step - loss: 0.1306 - accuracy: 0.9606\n",
      "Epoch 3/5\n",
      "5000/5000 [==============================] - 0s 51us/step - loss: 0.1159 - accuracy: 0.9638\n",
      "Epoch 4/5\n",
      "5000/5000 [==============================] - 0s 49us/step - loss: 0.1072 - accuracy: 0.9682\n",
      "Epoch 5/5\n",
      "5000/5000 [==============================] - 0s 49us/step - loss: 0.0902 - accuracy: 0.9752\n",
      "10000/10000 [==============================] - 0s 48us/step\n",
      "Epoch 1/5\n",
      "5000/5000 [==============================] - 1s 102us/step - loss: 0.0936 - accuracy: 0.9732\n",
      "Epoch 2/5\n",
      "5000/5000 [==============================] - 0s 50us/step - loss: 0.0821 - accuracy: 0.9746\n",
      "Epoch 3/5\n",
      "5000/5000 [==============================] - 0s 50us/step - loss: 0.0678 - accuracy: 0.9810\n",
      "Epoch 4/5\n",
      "5000/5000 [==============================] - 0s 49us/step - loss: 0.0665 - accuracy: 0.9812\n",
      "Epoch 5/5\n",
      "5000/5000 [==============================] - 0s 47us/step - loss: 0.0536 - accuracy: 0.9854\n",
      "10000/10000 [==============================] - 0s 48us/step\n",
      "Epoch 1/5\n",
      "5000/5000 [==============================] - 1s 107us/step - loss: 0.0557 - accuracy: 0.9838\n",
      "Epoch 2/5\n",
      "5000/5000 [==============================] - 0s 54us/step - loss: 0.0468 - accuracy: 0.9862\n",
      "Epoch 3/5\n",
      "5000/5000 [==============================] - 0s 53us/step - loss: 0.0351 - accuracy: 0.9914\n",
      "Epoch 4/5\n",
      "5000/5000 [==============================] - 0s 52us/step - loss: 0.0487 - accuracy: 0.9846\n",
      "Epoch 5/5\n",
      "5000/5000 [==============================] - 0s 53us/step - loss: 0.0358 - accuracy: 0.9892\n",
      "10000/10000 [==============================] - 0s 50us/step\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "  Dense(64, activation='relu', input_shape=(784,)),\n",
    "  Dense(64, activation='relu'),\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "\n",
    "experiments_results['less_data'] =\\\n",
    "learn_evaluate_n_times(n_times=5,\n",
    "               model=model,\n",
    "               data=(train_images[:5000], test_images, train_labels[:5000], test_labels),\n",
    "               optimizer='adam',\n",
    "               loss_func='categorical_crossentropy',\n",
    "               metrics=['accuracy'],\n",
    "               epochs=5,\n",
    "               batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'baseline': {'mean': 0.9713, 'std': 0.0021},\n",
       " 'more_layers': {'mean': 0.9678, 'std': 0.0038},\n",
       " 'more_neurones_in_layer': {'mean': 0.9728, 'std': 0.0042},\n",
       " 'more_neurones_in_layer_and_more_layers': {'mean': 0.9713, 'std': 0.0043},\n",
       " 'sigmoid_inside': {'mean': 0.9678, 'std': 0.0059},\n",
       " 'more_epochs': {'mean': 0.9725, 'std': 0.0018},\n",
       " 'larger_batch_size': {'mean': 0.9691, 'std': 0.0033},\n",
       " 'less_data': {'mean': 0.9157, 'std': 0.0095}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы по экспериментам по оценке влияния факторов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'baseline': {'mean': 0.9713, 'std': 0.0021},\n",
       " 'more_layers': {'mean': 0.9678, 'std': 0.0038},\n",
       " 'more_neurones_in_layer': {'mean': 0.9728, 'std': 0.0042},\n",
       " 'more_neurones_in_layer_and_more_layers': {'mean': 0.9713, 'std': 0.0043},\n",
       " 'sigmoid_inside': {'mean': 0.9678, 'std': 0.0059},\n",
       " 'more_epochs': {'mean': 0.9725, 'std': 0.0018},\n",
       " 'larger_batch_size': {'mean': 0.9691, 'std': 0.0033},\n",
       " 'less_data': {'mean': 0.9157, 'std': 0.0095}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как нейросети сложно строить обобщенные зависимости на малом объеме данных так и человеку. Эксперименты прикидочные, делать далеко идущие выводы по ним сложно и необоснованно, но тем не менее.\n",
    "\n",
    "- Кол-во слоев немного ухудшило качество и дисперсию, видимо, избыточное кол-во слоев приводит к несуществующим обобщениям и незначительному ухудшению качества.\n",
    "\n",
    "- Кол-во нейронов в слое качество улучшило - возможно, этот фактор, действительно, влияет положительно.\n",
    "\n",
    "- При одновременном увеличении кол-ва нейронов в слое и кол-ва слоев средняя метрика не изменилась, а разброс вырос, в таком контексте лучше выбрать базовую модель - из-за меньшего разброса и большей простоты модели.\n",
    "\n",
    "- Сигмоида вместо relu во внутреннем слое ощутимо ухудшила среднюю метрику и ощутимо увеличила дисперсию.\n",
    "\n",
    "- Увеличение кол-ва эпох сказалось положительно на среднем качестве и позволило добиться наименьшей из всех экспериментов дисперсии результатов.\n",
    "\n",
    "- Увеличение параметра batch_size метода fit() модели ухудшил качество.\n",
    "\n",
    "- Ну и ожидаемо значительное уменьшение объема обучающей выборки значительно и уменьшило значение метрики качества. Размер был уменьшен в порядка 10 раз. Эффект на качество - наибольший из всех экспериментов и очень ощутимый. Вероятно, это самый мощный фактор качества обучения, что соответствует общеизвестным фактам. В то же время почти очевидно, что дальнейшее добавление данных будет все меньше влиять на качество, т.е. можно предположить, что в некотором диапазоне приращение объема обучающей выборки будет давать взрывной рост качества, при обучении моделей следует, конечно, стремиться получить объем данных, лежащий за пределами этого диапазона справа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra experiments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Эксперимент: как данный тип нейросети сможет распознать изображение, переврнутое вниз головой если обучался на не перевернутой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Возвращает массив в обратном порядке.\n",
    "def resort_arr(arr):\n",
    "    return np.array([arr[-i] for i in range(1, len(arr) + 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = (train_images / 255) - 0.5\n",
    "test_images = (test_images / 255) - 0.5\n",
    "\n",
    "test_images_ = np.array([resort_arr(val) for val in test_images])\n",
    "\n",
    "train_images = train_images.reshape((-1, 784))\n",
    "test_images = test_images.reshape((-1, 784))\n",
    "test_images_ = test_images_.reshape((-1, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 3s 56us/step - loss: 0.3582 - accuracy: 0.8922\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 3s 52us/step - loss: 0.1879 - accuracy: 0.9425\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.1438 - accuracy: 0.9556\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.1218 - accuracy: 0.9623\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.1069 - accuracy: 0.9668\n",
      "10000/10000 [==============================] - 1s 50us/step\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.0980 - accuracy: 0.9688\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 3s 54us/step - loss: 0.0844 - accuracy: 0.9734\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 3s 54us/step - loss: 0.0797 - accuracy: 0.9746\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 3s 54us/step - loss: 0.0733 - accuracy: 0.9765\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 3s 54us/step - loss: 0.0671 - accuracy: 0.9786\n",
      "10000/10000 [==============================] - 1s 51us/step\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.0654 - accuracy: 0.9788\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 3s 58us/step - loss: 0.0599 - accuracy: 0.9804\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 0.0568 - accuracy: 0.9813\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.0540 - accuracy: 0.9819\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 0.0514 - accuracy: 0.9830\n",
      "10000/10000 [==============================] - 1s 60us/step\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.0503 - accuracy: 0.9833\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 0.0456 - accuracy: 0.9847\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 3s 58us/step - loss: 0.0436 - accuracy: 0.9855\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 3s 56us/step - loss: 0.0428 - accuracy: 0.9856\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 3s 57us/step - loss: 0.0416 - accuracy: 0.9859\n",
      "10000/10000 [==============================] - 1s 54us/step\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.0408 - accuracy: 0.9861\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 3s 57us/step - loss: 0.0369 - accuracy: 0.9874\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 3s 57us/step - loss: 0.0374 - accuracy: 0.9872\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 3s 58us/step - loss: 0.0357 - accuracy: 0.9880\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 3s 57us/step - loss: 0.0323 - accuracy: 0.9888\n",
      "10000/10000 [==============================] - 1s 58us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean': 0.332, 'std': 0.0214}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "  Dense(64, activation='relu', input_shape=(784,)),\n",
    "  Dense(64, activation='relu'),\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "\n",
    "reflected_results = learn_evaluate_n_times(n_times=5,\n",
    "               model=model,\n",
    "               data=(train_images, test_images_, train_labels, test_labels),\n",
    "               optimizer='adam',\n",
    "               loss_func='categorical_crossentropy',\n",
    "               metrics=['accuracy'],\n",
    "               epochs=5,\n",
    "               batch_size=32)\n",
    "\n",
    "reflected_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.3555 - accuracy: 0.8945\n",
      "10000/10000 [==============================] - 1s 57us/step\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "  Dense(64, activation='relu', input_shape=(784,)),\n",
    "  Dense(64, activation='relu'),\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "preds_ = learn_predict(model=model,\n",
    "               data=(train_images, test_images_, train_labels, test_labels),\n",
    "               optimizer='adam',\n",
    "               loss_func='categorical_crossentropy',\n",
    "               metrics=['accuracy'],\n",
    "               epochs=1,\n",
    "               batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 5, 1, ..., 9, 3, 9], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = np.argmax(preds_, axis=1)\n",
    "\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true</th>\n",
       "      <th>preds</th>\n",
       "      <th>match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   true  preds  match\n",
       "0     7      3      0\n",
       "1     2      5      0\n",
       "2     1      1      1\n",
       "3     0      0      1\n",
       "4     4      7      0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = pd.DataFrame({'true': test_labels, 'preds': preds})\n",
    "preds['match'] = 0\n",
    "preds.loc[preds['true'] == preds['preds'], 'match'] = 1\n",
    "preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Среднее качество предиктов в разрезе лейблов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true\n",
       "0    0.659184\n",
       "1    0.643172\n",
       "2    0.040698\n",
       "3    0.753465\n",
       "4    0.402240\n",
       "5    0.022422\n",
       "6    0.000000\n",
       "7    0.111868\n",
       "8    0.375770\n",
       "9    0.171457\n",
       "Name: match, dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.groupby('true')['match'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наиболее частый ответ модели в разрезе true значений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true\n",
       "0    0\n",
       "1    1\n",
       "2    3\n",
       "3    3\n",
       "4    9\n",
       "5    2\n",
       "6    9\n",
       "7    2\n",
       "8    3\n",
       "9    4\n",
       "Name: preds, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.groupby('true')['preds'].apply(lambda x: x.mode().iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Вывод по эксперименту с переворачиванием картинки вверх ногами:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При переворачивании картинок из тестового набора вверх ногами качество упало с 0,97 до 0,33. Очевидно, что модель не умеет распозонавать перевернутые изображения. Так же довольно очевидно, что в перевернутых цифрах тем не менее можно находить те же самые черты, что и в неперевернутых аналогах, об этом говорит качество 0,33, которое больше 0,1, которое соответсвует рандому.\n",
    "\n",
    "Построил в разрезе отдельных цифр среднее качество, а так же наиболее частый предикт модели в на каждой true цифре, т.е. на какую цифру какой результат выдавал модель чаще всего.\n",
    "\n",
    "Результаты довольно интересные и в то же время вполне предсказуемые в большинстве своем.\n",
    "\n",
    "На симметричные относительно горизонтали цифры (относительно конечно потому что там имеет место наклон) модель после переворачивания выдала наилучшие результаты: 0, 1 (в наборе почти все единички как палочки без зазубрины сверху), 3, 8. А вот по таким цифрам как 2,5,6,9 модель очень сильно ошибалась, как можно видеть из таблицы с модами - она принимала их за их перевернутые условные братья близнецы - 6 принимала за 9, 7 за 2. Но это прослеживаемые закономерности, в целом видимая картина шире этих закономерностей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Эксперимент: как нейросеть сможет обучаться и каково будет качество её предсказаний если входные данные кодировать по некоторой заданной маске."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = (train_images / 255) - 0.5\n",
    "test_images = (test_images / 255) - 0.5\n",
    "\n",
    "train_images = train_images.reshape((-1, 784))\n",
    "test_images = test_images.reshape((-1, 784))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Готовлю маску для кодирования изображения (пиксели при упощении 2D массива меняются местами рандомно, но всегда по одному алгоритму)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = {}\n",
    "\n",
    "arr = [i for i in range(784)]\n",
    "arr_ = arr.copy()\n",
    "shuffle(arr)\n",
    "\n",
    "for i in range(len(arr)):\n",
    "    mask[arr[i]] = arr_[i]\n",
    "\n",
    "def mask_image(arr_2d, mask):\n",
    "    result_arr = [0 for i in range(len(arr_2d))]\n",
    "    for i_new, i_old in mask.items():\n",
    "        result_arr[i_old] = arr_2d[i_new]\n",
    "    return np.array(result_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_ = np.array([mask_image(val, mask) for val in train_images])\n",
    "test_images_ = np.array([mask_image(val, mask) for val in test_images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 0.3517 - accuracy: 0.8961\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1849 - accuracy: 0.9438\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.1458 - accuracy: 0.9549\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.1207 - accuracy: 0.9620\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1061 - accuracy: 0.9671\n",
      "10000/10000 [==============================] - 1s 68us/step\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 0.0954 - accuracy: 0.9702\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.0862 - accuracy: 0.9722\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.0787 - accuracy: 0.9749\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.0700 - accuracy: 0.9776\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.0683 - accuracy: 0.9780\n",
      "10000/10000 [==============================] - 1s 69us/step\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.0650 - accuracy: 0.9784\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.0585 - accuracy: 0.9807\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.0552 - accuracy: 0.9817\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.0543 - accuracy: 0.9823\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.0494 - accuracy: 0.9837\n",
      "10000/10000 [==============================] - 1s 70us/step\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.0491 - accuracy: 0.9837\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.0452 - accuracy: 0.9855\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.0416 - accuracy: 0.9858\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.0424 - accuracy: 0.9854\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 0.0396 - accuracy: 0.9866\n",
      "10000/10000 [==============================] - 1s 70us/step\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.0385 - accuracy: 0.9871\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.0364 - accuracy: 0.9876\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.0343 - accuracy: 0.9883\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.0337 - accuracy: 0.9885\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.0329 - accuracy: 0.9887\n",
      "10000/10000 [==============================] - 1s 70us/step\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "  Dense(64, activation='relu', input_shape=(784,)),\n",
    "  Dense(64, activation='relu'),\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "\n",
    "experiments_result =\\\n",
    "learn_evaluate_n_times(n_times=5,\n",
    "               model=model,\n",
    "               data=(train_images_, test_images_, train_labels, test_labels),\n",
    "               optimizer='adam',\n",
    "               loss_func='categorical_crossentropy',\n",
    "               metrics=['accuracy'],\n",
    "               epochs=5,\n",
    "               batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Вывод по эксперименту с запограммированным перемешиванием."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запрограммированное перемешивание пикселей, т.е. любой алгоритм перемешивания пикселей в изображении (главное чтобы он был одинаковым для всех изображений) никак не влияет на качество обучения модели. С одной стороны предсказуемо, с другой стороны ни разу не помогает в проникновении внутрь черного ящика нейросети и понимании того, как именно нейросеть классифицирует, учится и узнает классы. Получается, геометрическое расположение пикселей относительно друг друга (близость и т.д.) никак не влияет на возможность и сам процесс узнавания моделью."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нда, похоже, мои выкрутасы с прогонами обучения по 5 раз на модели делают не совсем то, что я ожидал. Похоже не 5 моделей с нуля обучаются, а одна модель дообучается 5 раз. Ну, в целом сильно на рассуждения это не влияет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Интересные функциональные возможности и команды Keras из документации:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давно мне было интересно - когда читаешь все эти статьи про мега-нейросети, которые обучались месяц - неужели они поставили на месяц и ушли \"пить чай\", а если ошибка, а если баг, а если надо как-то замерить промежуточные результаты и что-то скорретировать. Похоже, коллбеки - та функциональность, которая позволяет разруливать все эти вопросы и процессы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Только почему-то пример из документации падает с ошибкой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Keras model to add callbacks to\n",
    "def get_model():\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(1, input_dim=784))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.RMSprop(learning_rate=0.1),\n",
    "        loss=\"mean_squared_error\",\n",
    "        metrics=[\"mean_absolute_error\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Load example MNIST data and pre-process it\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 784).astype(\"float32\") / 255.0\n",
    "x_test = x_test.reshape(-1, 784).astype(\"float32\") / 255.0\n",
    "\n",
    "# Limit the data to 1000 samples\n",
    "x_train = x_train[:1000]\n",
    "y_train = y_train[:1000]\n",
    "x_test = x_test[:1000]\n",
    "y_test = y_test[:1000]\n",
    "\n",
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Starting training; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Stop training; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Start epoch {} of training; got log keys: {}\".format(epoch, keys))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"End epoch {} of training; got log keys: {}\".format(epoch, keys))\n",
    "\n",
    "    def on_test_begin(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Start testing; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_test_end(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Stop testing; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_predict_begin(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Start predicting; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_predict_end(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Stop predicting; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Training: start of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Training: end of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_test_batch_begin(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Evaluating: start of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Evaluating: end of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_predict_batch_begin(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Predicting: start of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_predict_batch_end(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Predicting: end of batch {}; got log keys: {}\".format(batch, keys))\n",
    "        \n",
    "model = get_model()\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=128,\n",
    "    epochs=1,\n",
    "    verbose=0,\n",
    "    validation_split=0.5,\n",
    "    callbacks=[CustomCallback()],\n",
    ")\n",
    "\n",
    "res = model.evaluate(\n",
    "    x_test, y_test, batch_size=128, verbose=0, callbacks=[CustomCallback()]\n",
    ")\n",
    "\n",
    "res = model.predict(x_test, batch_size=128, callbacks=[CustomCallback()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
